---
source_url: https://ai-data-base.com/archives/100514
captured_at: 2026-01-18T19:15:23.313287
title: "LLM-as-a-Judgeを使う際の盲点　評価精度は採点スケールで大きく変わる"
publish_date: 2026.01.12
tags: ["実証", "LLM", "手法\n426", "実証\n137", "分析\n54", "サーベイ\n37", "ベンチマーク・リソース\n22", "テクニカルレポート\n15", "ポジション\n8", "LLM\n659", "プロンプト技術\n156", "エージェント\n128", "コーディング\n56", "RAG\n50", "安全性\n39", "ペルソナ・シミュレーション\n36", "オープンソース\n25", "マルチモーダル\n23", "画像認識\n20", "セキュリティ\n16", "ハルシネーション\n16", "ファインチューニング\n16", "画像生成\n9", "音声\n8", "医療・ヘルスケア\n33", "政治・社会\n29", "エンタメ・アート\n23", "金融・経済\n10", "SE\n9", "教育・キャリア\n9", "製造・デザイン\n9", "ロボット\n6"]
conversion_method: browser_subagent_v1
is_premium: unknown
---

# LLM-as-a-Judgeを使う際の盲点　評価精度は採点スケールで大きく変わる

![image](data:image/png;base64,...)

本記事では、LLMを自動評価者として使うときに、あまり意識されてこなかった要因に注目した研究を紹介します。

LLMによる評価は安定しにくいという問題もあります。プロンプトの書き方を少し変えただけで、同じ回答でも点数が変わってしまうことがあります。このようなばらつきは、評価結果をそのまま信頼しにくくする原因になります。

この安定性の問題に対して、これまで主にプロンプトの工夫や学習方法の改善に取り組んできました。しかし、採点に使うスケールが、LLMの判断にどのような影響を与えるのかについて、体系的に調べた研究はほとんどありませんでした。

![image](https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100514-1024x576.jpg)

本記事の関連研究

* コスト削減の切り札か、信頼性の落とし穴か　LLM-as-Judgeの実用性を検証する
* LLMによるスコア評価のクセを把握しよう
* LLMを「評価者」として活用する『LLM-as-a-judge』の基本

## LLM-as-a-Judgeの不安定さはプロンプト設計だけでなく採点スケールにも原因がある可能性

LLMを評価者として使う「LLM-as-a-Judge」という方法が、いま急速に広がっています。人の代わりにLLMが回答の出来を採点する手法で、翻訳の質を比べたり、チャットボットの返答を評価したりと、幅広い場面で使われています。人手で評価するよりもコストがかからず、短時間で結果が得られることから、多くの現場で当たり前の選択肢になりつつあります。

AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAbwEAlwAB/pkcvwAAAABJRU5ErkJggg==)

これは、nMAEが平均的にどれくらい点数が離れているかを見るのに対し、ICCは項目ごとの違いがきちんと反映されているかまで見ているためです。平均的なズレが小さくても、評価のばらつき方が適切でなければ、ICCは低くなります。逆に、ICCが高い場合は、点数の大きさだけでなく、項目ごとの違いも正しく捉えられていると考えられます。

このため、本研究では、どちらか一方だけでなく、両方の指標をあわせて見ることが重要だと結論づけられています。

## 採点スケールの設計は固定された慣習ではなく制御可能なパラメータとして扱うべき

本研究の結果から、LLM-as-a-Judgeを使ううえで見逃せないポイントが整理できます。

まず6つのベンチマークと複数のLLMを使った検証を通して、採点スケールの数値範囲は、単にプロンプトで選べるオプションの一つではないことがはっきりしました。とくに、正解がはっきりしない主観的な評価タスクでは、LLMのスコアはスケールを変えると同じようには振る舞いません。LLM同士や人間同士の評価が安定している場合でも、人間とLLMの一致度は、どのスケールを使うかによって大きく変わります。

すべてのタスクをまとめて見ると、0から5のスケールが、人間の評価とのあいだで最も強い一致を示しています。一方で、0から10のスケールは一貫して最も相性が悪い結果になっています。この並びは、LLMの温度パラメータを変えても崩れませんでした。

同時に注意すべき点として、全体をまとめた信頼性指標（つまり統合値）だけを見ると、重要な違いが見えなくなってしまう問題も指摘されています。
LLMの評価が安定しているように見えるのは、判断しやすい客観的なタスクが全体のばらつきを強く支配しているためです。このことは、信頼性や一致度を、タスクごとや評価者のグループごとに分けて確認する必要があることを示しています。

こうした結果を総合すると、評価が難しく、結果のばらつきが大きい生成タスクを扱う場合には、採点スケールの設計そのものを重要な調整項目として扱うべきだと言えます。スケール選択や詳細な分析は、後回しにされがちな慣習ではなく、LLM評価のプロトコルに最初から組み込むべき要素だというのが結論です。

## まとめ

本記事では、LLM-as-a-Judgeにおける採点スケールの影響を体系的に検証した研究を取り上げました。

6つのベンチマークと6種類のLLMを用いた実験の結果、0-5スケールが人間との一致度を最も高め、0-10スケールが最も低いことが明らかになっています。また、主観的なタスクではLLM同士の評価一貫性が低下すること、全体の信頼性指標がタスクごとの差異を覆い隠してしまうことも示されています。

総じて、採点スケールを固定された慣習ではなく、意識的に設計すべきパラメータとして捉える必要があることが示唆されました。

本記事の関連研究

* コスト削減の切り札か、信頼性の落とし穴か　LLM-as-Judgeの実用性を検証する
* LLMによるスコア評価のクセを把握しよう
* LLMを「評価者」として活用する『LLM-as-a-judge』の基本