---
source_url: https://ai-data-base.com/archives/65652
captured_at: 2026-01-18T12:55:12.409Z
title: "マルチモーダルLLMで画像分析する際、画像に「ドットマトリックス」を重ねるだけで認識精度が大きく向上 - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-23T16:52:23+09:00
conversion_method: Readability+Turndown
file_hash: bb70b9625b474bc0
---

大規模マルチモーダルモデルは、複雑な推論が必要なタスクではまだまだパフォーマンスが限られています。そこで、画像上に点のマトリックスを重ね、各点に座標を割り当てることで精度を向上する手法『SCAFFOLD』が考案されました。

実験では空間推論、視覚的理解、幻覚の検出など、様々なベンチマークでSCAFFOLDの有効性が示されました。

![](https://ai-data-base.com/wp-content/uploads/2024/03/AIDB_-65652-1024x576.jpg)

**参照論文情報**

*   タイトル：Scaffolding Coordinates to Promote Vision-Language Coordination in Large Multi-Modal Models
*   機関：Tsinghua University
*   著者：Xuanyu Lei, Zonghan Yang, Xinrui Chen, Peng Li, Yang Liu

**本記事の関連研究**：

*   [Gemini Pro 対 GPT-4V、画像認識能力でどちらが優秀なのか](https://ai-data-base.com/archives/61286)
*   [マルチモーダルLLMの技術や開発トレンド、26種類のモデル例を網羅的にまとめた報告](https://ai-data-base.com/archives/63257)
*   [スクショからHTMLとCSSのコードをLLMが生成する『Design2Code』タスク、プロンプト手法やファインチューニングで高い性能を確認](https://ai-data-base.com/archives/65294)
*   [AGIを見据えて専門家レベルの問題を集めたベンチマーク「MMMU」、GPT-4VやGemini Ultraでも正解率6割未満](https://ai-data-base.com/archives/61463)

## 背景

GPT-4Vなどの大規模マルチモーダルモデルは様々なタスクで優れた性能を示しています。言語モデルの高度な推論能力を活用し、現実のシナリオへの応用が期待されています。

しかし、現在は複雑な推論を行う際に性能が限られています。例えば、空間推論タスクでは、画像中の様々な情報源の関係を明らかにする必要があります。つまり、正確な視覚認識と言語理解のオーケストレーションが求められるのです。

マルチモーダルモデルのレベルを上げるためにこれまでに大きく2つのアプローチが取られてきました。

一つ目はInstruction Tuningで、高品質な画像テキストペアで追加学習する手法です。しかし、大量の計算リソースを消費するため、柔軟性に欠けます。

二つ目はPromptingで、Chain-of-Thoughtなどが代表例です。ただし、テキストpromptは探求されているものの、Visual promptingはあまり検討されていません。

今、シンプルで汎用的なVisual promptingの手法が求められています。そこで新たに考案されたのが『SCAFFOLD』です。

以下で詳細を紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.