---
source_url: https://ai-data-base.com/archives/64001
captured_at: 2026-01-18T13:23:05.719Z
title: "ファインチューニングデータが十分に大きい場合、タスク性能向上に追加の事前学習は不要の可能性 Googleなどによるスケーリング則の実験から - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T21:08:41+09:00
conversion_method: Readability+Turndown
file_hash: 2ef3ad4f45b9912e
---

Googleとスタンフォード大学の研究者らは、下流タスク（機械翻訳などの具体的なタスク）における大規模言語モデルのスケーリング則を調査しました。その結果、新しい知見がいくつか得られています。

スケーリング則とはモデルの学習データ量やサイズの増加によって性能がどう変化するのかを説明するものです。

本研究においてはファインチューニングに対する洞察やデータセットの選択と評価などに関する考え方なども紹介されており、事前学習におけるスケーリング則以外にも示唆に富む情報が含まれています。

![](https://ai-data-base.com/wp-content/uploads/2024/02/AIDB_64001-1024x576.jpg)

**参照論文情報**

*   タイトル：Scaling Laws for Downstream Task Performance of Large Language Models
*   著者：Berivan Isik, Natalia Ponomareva, Hussein Hazimeh, Dimitris Paparas, Sergei Vassilvitskii, Sanmi Koyejo
*   所属：Stanford University, Google Research
*   URL：[https://doi.org/10.48550/arXiv.2402.04177](https://doi.org/10.48550/arXiv.2402.04177)

**本記事の関連研究**：

*   [LLMに外部知識を取り入れる2つの手法、ファインチューニングとRAGを比較した実験結果](https://ai-data-base.com/archives/63401)
*   [GPT-4レベルの質問応答タスク性能をオープンソースモデルのLlama 2で実現する「ChatQA」NVIDIAが開発](https://ai-data-base.com/archives/63211)
*   [1.1Bパラメータの小さなモデルを巨大データ（約3兆トークン）で訓練したモデル『TinyLlama』が、比較的優秀な性能を発揮](https://ai-data-base.com/archives/61914)
*   [人間のカリキュラム教育のような学習でLLMの性能は向上するとの報告](https://ai-data-base.com/archives/61555)

## 背景

LLMのモデル開発における設計指針（例：トレーニングデータの規模、アーキテクチャ）の材料として「スケーリング則」は重要視されているものの一つです。

スケーリング則とは、モデルのサイズやデータ量が増加するにつれて、そのパフォーマンスや効率がどのように変化するかを表す法則です。LLMにおけるスケーリング則では、モデルのパラメータ数、訓練データの量、計算資源が増加するにつれて一般的にモデルの性能は改善し、改善の度合いは次第に減少するなどのことが分かっています。

これまでの研究では、主に事前学習データと上流タスクのクロスエントロピー（予測と実際の分布との差異を測る尺度）などに対するスケーリング則に焦点が当てられてきました。しかし実用的にLLMを使う場合は、LLMは具体的な下流タスクに強くなるよう転移学習（下流タスク向けにファインチューニング）されることも多いです。

ここで、LLMにおける上流タスクとは言語理解などの基本性能を指し、下流タスクとは例えば機械翻訳などの細分化された具体的なタスクを意味します。

なお、事前学習データを増やせば上流下流ともにタスク性能が必ず上がるわけではなく、画像認識の分野では時には性能が下がる場面もあると報告されています。  
なおファインチューニングのデータ量におけるスケーリング則も研究されてはいますが、十分には調べられていません。

そこでGoogleなどの研究者らは、下流タスク（機械翻訳）における事前学習データの規模とファインチューニング後のタスク性能の関係性を調査し、スケーリング則の知見を新しく得ることに成功しました。

## 下流タスクにおけるスケーリング則

研究者らは調査の結果、ファインチューニング後の下流タスクにおけるスケーリング則を、以下のようにまとめました。先に結論から整理します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.