---
source_url: https://ai-data-base.com/archives/63153
captured_at: 2026-01-18T13:22:28.506Z
title: "既存のLLMを融合させて強力なモデルを作る手法「知識融合」 - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T21:08:44+09:00
conversion_method: Readability+Turndown
file_hash: ec888bb2628226f3
---

LLMを一から作るには膨大な時間とコストがかかる上に、似たようなモデルができてしまうリスクもあります。

そこで今回研究者が提案しているのが「知識融合（knowledge fusion）」という手法です。すでに存在している事前学習済みLLMを組み合わせ、もっと強力なモデルを作るアプローチです。

実験では、さまざまなタスクで成功が確認されています。

![](https://ai-data-base.com/wp-content/uploads/2024/01/AIDB_63153-1024x576.jpg)

**参照論文情報**

*   タイトル：Knowledge Fusion of Large Language Models
*   著者：Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, Shuming Shi
*   所属：Sun Yat-sen University, Tencent AI Lab
*   URL：[https://doi.org/10.48550/arXiv.2401.10491](https://doi.org/10.48550/arXiv.2401.10491)
*   GitHub：[https://github.com/fanqiwan/FuseLLM](https://github.com/fanqiwan/FuseLLM)

**本記事の関連研究**：

*   [オープンソースLLMのMixtral 8x7B　GPT-3.5に匹敵する性能を示す高効率モデル](https://ai-data-base.com/archives/62480)
*   [LLMの知識を狙い撃ちして変更・修正する「知識編集（Knowledge Editing）」](https://ai-data-base.com/archives/61831)
*   [1.1Bパラメータの小さなモデルを巨大データ（約3兆トークン）で訓練したモデル『TinyLlama』が、比較的優秀な性能を発揮](https://ai-data-base.com/archives/61914)
*   [あらゆるLLMを「使い心地」基準でバトルさせる便利なプラットフォーム『Chatbot Arena：チャットボットアリーナ』](https://ai-data-base.com/archives/61080)
*   [「ChatGPTの1周年を記念して」、オープンソースLLMがChatGPTにどこまで追いついているか体系的調査報告](https://ai-data-base.com/archives/59713)

## 背景

仮に十分な性能を持つLLMを一から作ろうと考えた場合、膨大な量のデータや高度な技術と知識、そして大量の計算資源（GPUなど）が必要になります。開発の過程でエネルギー消費や環境への影響も甚大になってしまいます。

また既存のLLMはさまざまなタスクで似たような能力を持っていることも報告されています。そのため研究者らは今回、新しいモデルを作る時には、すでに存在するLLMを融合させたほうが効率的に強いモデルを開発できるのではないかと考えました。

なお、これまでにもニューラルネットワークモデルを組み合わせる研究はいくつか行われてきました。例えば「アンサンブル法」という、複数のモデルによる出力を合わせる手法や、複数のネットワークを一つに合わせる「マージング（重みのマージ）」が試されてきました。しかしこれら既存の手法はLLMには向いていないとされています。サイズが大きく、メモリや処理時間の要求が高いためだと言われています。

今回研究者らは、複数の異なるLLMが生成する確率分布（入力に対してどんな出力を行うかの確率）を混ぜることで、各LLMの知識や強みを単一のLLMに移すことを目指す手法「知識融合」を考案しました。理論通りにいけば、混ぜ合わせられる前の各モデルよりも強くなるアプローチです。

![](https://ai-data-base.com/wp-content/uploads/2024/01/AIDB_63153_1-1024x438.png)

なお本手法は、一般的な知識蒸留とは違って、対象となるモデルのサイズに制約はないそうです。知識蒸留とは、大きなモデルから学んだ知識を小さなモデルに伝える手法で、もとのモデルの性能を超えることはあまりないと言われています。実験では、知識蒸留との直接的な比較も行われています。

以下では、知識融合の方法論と実験結果などを紹介します。

## 知識融合の方法論

研究者らは、

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.