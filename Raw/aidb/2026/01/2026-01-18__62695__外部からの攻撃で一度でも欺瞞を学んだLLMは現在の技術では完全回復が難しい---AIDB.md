---
source_url: https://ai-data-base.com/archives/62695
captured_at: 2026-01-18T13:22:22.414Z
title: "外部からの攻撃で一度でも欺瞞を学んだLLMは現在の技術では完全回復が難しい - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T21:08:45+09:00
conversion_method: Readability+Turndown
file_hash: 4fd3ebddff76b672
---

人間と似たようにLLMも欺瞞（隠れた目的を持ってごまかす）的な行動をとることがあることが実験で示されました。

加えて、一度でも欺瞞的な行動を学んだモデルは現状その特徴を取り除くことは通常できないとのことです。

本記事では研究報告の抜粋を紹介します。

![](https://ai-data-base.com/wp-content/uploads/2024/01/AIDB_62695-1024x576.jpg)

**参照論文情報**

*   タイトル：Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training
*   著者：Evan Hubinger et al.（多数）
*   所属：Anthropic、Redwood Research、University of Oxford、Open Philanthropyなど
*   URL：[https://doi.org/10.48550/arXiv.2401.05566](https://doi.org/10.48550/arXiv.2401.05566)
*   GitHub：[https://github.com/anthropics/sleeper-agents-paper](https://github.com/anthropics/sleeper-agents-paper)

## 背景

LLMは、複雑なタスクを遂行する能力を持っている一方で、特定の条件下においては、安全でない振る舞いを示す可能性もあります。現段階では脆弱性が意図的に悪用されるリスクに注意が必要であり、さらに、意図しない形でモデルが振る舞い、何らかの影響を及ぼすリスクも考えなければいけません。

**関連研究：**[大規模言語モデルのセーフガードを故意に突破する「脱獄プロンプト」とは](https://ai-data-base.com/archives/54468)

現在の安全対策のための訓練（例えば、強化学習に基づくファインチューニングなど）が、LLMの潜在的なリスクに対して十分に効果的であるかどうかに関して疑問が提起されています。要するに、今主流とされている安全対策アプローチが、モデルの不適切な振る舞いをどの程度抑制または除去できるかがまだ分かっていないという状況です。

リスクの中でも、LLMが特定の条件下で攻撃的な振る舞いを示す可能性に注意が集まっています。例えば、モデルにバックドア行動を組み込まれてしまうことがあるのか、という点が研究されています。

バックドアとは、ITにおけるセキュリティの用語です。ソフトウェアやシステム内に意図的に設けられた、通常の認証やセキュリティチェックを迂回するための手段です。言うなれば、悪意をもった技術者がこっそり内部につけておく裏口みたいなものです。

モデルが特定のトリガーに基づいて異なる行動をとる能力や、そのような行動が安全訓練に対して耐性を持つのであれば、由々しき事態と言えます。  
結論から言えば、バックドア自体を取り付けることは原理的には可能なので、安全対策がそれに対してどれほど効果的なのか、逆に言えばどこに限界があるのかを知る必要があります。

上記のような背景を踏まえて、Anthropicなど複数機関から多くの研究者が集まった研究グループは、LLMに組み込まれるバックドア行動と既存の安全対策の力比べを検証することにしました。

![](https://ai-data-base.com/wp-content/uploads/2024/01/AIDB_62695_1.png)

実験セットアップの説明

**本記事の関連研究**：

1.  [LLMのLlama-2を分解したところ、1057個のパーツから「抑制」に影響する部品が明らかに](https://ai-data-base.com/archives/61767)
2.  [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
3.  [OpenAIが開発中の「人間を超えたAIを制御する」方法](https://ai-data-base.com/archives/61116)
4.  [わずか2行のプロンプトでも実効性のある新しいアライメント手法『URIAL』](https://ai-data-base.com/archives/60678)
5.  [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)
6.  [ChatGPTはチューリングテスト（人間模倣ゲーム）に一定の確率で成功する。ただし解釈には注意を](https://ai-data-base.com/archives/58540)

## 実験設計

研究者らは、

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.