---
source_url: https://ai-data-base.com/archives/100329
captured_at: 2026-01-18T20:04:08.475247
title: "企業がLLMを導入する際に知っておくべきジェイルブレイク対策の現在地"
publish_date: 2026.01.07
tags: ["実証", "LLM", "安全性", "手法\n426", "実証\n137", "分析\n54", "サーベイ\n37", "ベンチマーク・リソース\n22", "テクニカルレポート\n15", "ポジション\n8", "LLM\n659", "プロンプト技術\n156", "エージェント\n128", "コーディング\n56", "RAG\n50", "安全性\n39", "ペルソナ・シミュレーション\n36", "オープンソース\n25", "マルチモーダル\n23", "画像認識\n20", "セキュリティ\n16", "ハルシネーション\n16", "ファインチューニング\n16", "画像生成\n9", "音声\n8", "医療・ヘルスケア\n33", "政治・社会\n29", "エンタメ・アート\n23", "金融・経済\n10", "SE\n9", "教育・キャリア\n9", "製造・デザイン\n9", "ロボット\n6"]
conversion_method: browser_subagent_v1
is_premium: unknown
---

本記事では、LLMに対するジェイルブレイク攻撃と、その防御方法を体系的に調べた事例を紹介します。

LLMが企業の現場で広く使われるようになるにつれ、これらを悪用しようとする攻撃手法も急速に進化してきました。中でもジェイルブレイクと呼ばれる攻撃は、モデルに備わった安全機能を回避し、有害な内容を出力させるものです。これまでの多くの調査では、この種の攻撃が高い確率で成功することが報告されてきました。

ただし、ここで見過ごされがちな点があります。実際の商用サービスでは、言語モデルそのものの安全機構に加えて、入力や出力を監視するセーフティフィルターが組み込まれているのが一般的です。では、こうした追加の防御層を含めて考えた場合、ジェイルブレイク攻撃はどれほど現実的な脅威なのでしょうか。また、防御する側は本当に有利な立場にあると言えるのでしょうか。

![](https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-1024x576.png)

<img fetchpriority="high" decoding="async" width="1024" height="576" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-1024x576.png" alt="" class="wp-image-100332" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-1024x576.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-300x169.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-768x432.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-1536x864.png 1536w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-400x225.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-800x450.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329-1200x675.png 1200w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329.png 1600w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />

本記事の関連研究

- RAGシステムにおけるセキュリティ対策の投資対効果
- LLMエージェントのベースモデルに何を使う？安全性ランキング調査結果
- Copilotサービスを使う上で気を付けたいプロンプトインジェクションによる機密情報漏洩リスク

## 背景

LLMは、非常に速いペースで私たちの仕事や生活の中に入り込んできました。文章作成やプログラミングの補助、カスタマーサポートの自動化など、利用される場面は急速に広がっています。

一方で、こうした強力な技術には必ずリスクも伴います。中でも、LLMが悪意のある指示に応じて有害な内容を生成してしまうのは深刻な課題です。この問題に対応するため、各社は安全性アラインメントと呼ばれる対策を導入しています。モデルが危険な要求を見分け、適切に拒否できるよう学習させる仕組みです。

ところが、この安全対策をすり抜ける攻撃手法も次々と登場しています。ジェイルブレイクと呼ばれる攻撃で、巧妙な言い回しや文脈を用いてモデルを誤認させ、本来は拒否されるべき有害な出力を引き出します。これまでの研究では、こうした攻撃が高い成功率を示すことが繰り返し報告され、LLMの安全性に対する懸念が強調されてきました。

しかし、実際に企業がLLMを運用する際には、モデルそのものの安全機構に加えて、入力や出力を検査するコンテンツフィルターと呼ばれる追加の防御が組み込まれるのが一般的です。ユーザーの入力内容を事前に確認し、モデルの出力結果も後からチェックすることで、リスクを抑える仕組みになっています。

これまでの調査はモデル単体を対象とした評価にとどまっており、こうした実際の運用環境に近い条件での検証はほとんど行われてきませんでした。

そこで本記事では、代表的なジェイルブレイク攻撃手法と主要なセーフティフィルターを組み合わせて体系的に評価した研究を取り上げ、攻撃と防御が現在どの段階にあるのかを明らかにしていきます。

ここから限定コンテンツ

### 忙しい人向けに、重要なポイント5選

1. セーフティフィルターを組み合わせると、従来報告されていたジェイルブレイク攻撃の成功率は大幅に低下する
2. 意味的なステルス性を考慮していない攻撃手法は、既存のフィルターによって容易に検出される
3. 推論能力を持つモデルが高い検出性能を示し、攻撃手法を問わず安定した防御力を発揮した
4. 現行のフィルターは有害コンテンツの見逃しを防ぐことを優先しており、無害な入力を誤ってブロックする課題が残っている
5. LlamaGuardやOpenAI APIなど主要なフィルターは低コストかつ低遅延で運用可能であり、実用的な導入障壁は低い

参照文献情報

- タイトル：Jailbreaking Attacks vs. Content Safety Filters: How Far Are We in the LLM Safety Arms Race?
- URL：https://doi.org/10.48550/arXiv.2512.24044
- 著者：Yuan Xin, Dingfan Chen, Linyi Yang, Michael Backes, Xiao Zhang
- 所属：CISPA Helmholtz Center for Information Security, Max Planck Institute for Intelligent Systems, Southern University of Science and Technology

## 実運用環境を再現した評価設計

研究者らはジェイルブレイク攻撃の成否を、モデル単体ではなく実際の運用環境に近い条件で評価しました。どのような枠組みで検証が行われたのかを順に見ていきます。

### 攻撃の成否はJudgeモデルが自動判定する

まず、ジェイルブレイク攻撃の基本的な仕組みを整理します。

攻撃者の狙いは、巧妙に作られた入力をLLMに与えることで、本来なら拒否されるはずの有害な内容を出力させることです。そこで、攻撃が成功したかどうかは、判定用のモデルによって判断します。生成された出力が攻撃者の意図する有害な目標に沿っているかを自動的に評価します。

これまでの研究では、このJudgeによる判定のみで攻撃の成否が決められてきました。しかし、実際のサービスではそれだけでは不十分です。現場には、さらに別の防御が組み込まれています。

### 入力と出力の二段階で有害コンテンツをブロックする

実運用の環境では、LLMの前後にコンテンツフィルターが配置されるのが一般的です。このフィルターには、大きく分けて二つの役割があります。

一つ目は入力フィルターです。ユーザーの入力がLLMに渡される前に、その内容が危険でないかをチェックします。この段階で問題があると判断されれば、入力はそのまま遮断され、LLMには届きません。

二つ目は出力フィルターです。LLMが生成した回答をユーザーに返す前に、その内容を検査します。仮にLLMが有害な出力を生成してしまっても、ここで検出されればユーザーの目に触れることはありません。

つまり、ジェイルブレイク攻撃が本当に成功したと言えるのは、入力フィルターを通過し、LLMから有害な出力を引き出し、さらに出力フィルターもすり抜けた場合だけです。

### 汎用LLMから専用モデルまで6種類のフィルターを検証

現在利用されている代表的なセーフティフィルターが幅広く検証されました。

まず、汎用的なLLM。GPT-4（すでに過去のモデルとなりましたが）やO3のようなモデルは、大規模なデータで学習され、安全性に関する調整も行われているため、追加の訓練なしでも有害なコンテンツを見分ける力を持っています。O3のような推論能力が高いモデルは、とくに文脈を踏まえた判断が可能です。

次に、安全性検出に特化して追加学習されたモデルによるフィルターです。

セーフティフィルター役割・特徴[LlamaGuard](https://github.com/meta-llama/PurpleLlama)暴力やヘイトスピーチなど、あらかじめ定義されたカテゴリに基づいて有害コンテンツを分類するよう設計されている[PromptGuard](https://github.com/meta-llama/PurpleLlama/tree/main/Prompt-Guard)悪意のある入力やジェイルブレイク攻撃そのものを検出することに重点を置いて訓練されたモデル[InjecGuard](https://github.com/InjecGuard/InjecGuard)PromptGuardで課題となっていた過剰な検出を抑えることを目的に改良されたモデル

また、OpenAIが提供するContent Moderation APIも評価対象に含まれています。

さらに、少し異なる発想の手法としてGradSafeがあります。LLM内部で計算される勾配、つまりモデルのパラメータがどの方向に変化しようとしているかを分析することで、ジェイルブレイクを検出する方法です。有害な入力を処理する際には、特徴的な勾配のパターンが現れるという性質を利用しています。

### 417件の有害プロンプトでパス率を測定

研究チームは、既存の複数のベンチマークを統合し、意味的な重複するサンプルを除外することで、417件の有害なプロンプトからなる評価用データセットを作成しました。加えて、それぞれの有害プロンプトに対応するトピックについて、無害なプロンプトも417件用意しています。

評価指標としては、まず従来と同じ「攻撃成功率」が計測されます。フィルターを考慮せず、Judgeの判定だけで成功かどうかを判断するものです。
次に、入力段階でどれだけ検出できたか、出力段階でどれだけ検出できたかが個別に測定されます。そして最も重要なのが、入力フィルターと出力フィルターの両方を通過してしまった割合、いわゆるパス率です。この値が低いほど、防御が効果的に機能していることを示します。

### 最適化からマルチターンまで10種類の攻撃手法を網羅

多様なジェイルブレイク攻撃手法が評価されました。

攻撃手法概要[GCG](https://arxiv.org/abs/2307.15043)勾配情報などを使って敵対的な接尾辞を探索し、拒否されるはずの要求でもモデルが応答しやすい形に最適化する手法[AutoDAN](https://arxiv.org/abs/2310.04451)遺伝的アルゴリズムを用いてプロンプトを進化させ、ステルス性の高いジェイルブレイク文を自動生成する手法[Adaptive Attacks](https://arxiv.org/abs/2404.02151)防御の仕様や挙動に合わせて攻撃を調整し、より高い成功率でガードレールを突破しようとする適応型の攻撃手法[PAIR](https://arxiv.org/abs/2310.08419)別のLLMを攻撃者として使い、少ない試行回数でジェイルブレイク用プロンプトを反復的に改善していく手法[TAP](https://arxiv.org/abs/2312.02119)木構造探索と枝刈りで候補プロンプトを効率よく絞り込み、ブラックボックスの対象LLMを自動的にジェイルブレイクする手法[AdvPrompter](https://arxiv.org/abs/2404.16873)人間が読める自然な接尾辞を高速に生成し、意味を保ったまま安全機構を回避しやすくする攻撃手法[DeepInception](https://arxiv.org/abs/2311.03191)多層の架空シナリオや役割設定を重ねてモデルを誘導し、安全判断をすり抜ける難読化系の手法[ReNeLLM](https://arxiv.org/abs/2311.08268)プロンプトの書き換えとシナリオ埋め込みを組み合わせ、攻撃意図を間接化して突破を狙う枠組み[DrAttack](https://arxiv.org/abs/2402.16914)有害な指示を分解し、無害に見える形で再構成して提示することで検出や拒否を回避しやすくする手法[CodeChameleon](https://arxiv.org/abs/2402.16717)関数やコード補完などの形式に偽装しつつ、個別の変換（暗号化的な変形）で整列時に見えにくい入力へ変える手法[Crescendo](https://arxiv.org/abs/2404.01833)一見無害な会話から始め、複数ターンで段階的に目的へ近づけて最終的に突破するマルチターン攻撃

## セーフティGPT-4oが使用されました。

### フィルター導入でパス率は大幅に低下する

最も重要な結果は、セーフティフィルターが高い効果を示した点です。これまでのジェイルブレイク研究では、攻撃成功率が90%を超えるケースも珍しくありませんでした。しかしセーフティフィルターを組み込むことで状況は大きく変わります。

PromptGuardやO3といったフィルターは、入力段階でおおむね70%から100%という高い検出率を示しました。その結果、多くの攻撃手法においてパス率は5%未満まで低下しています。つまり、フィルターをすり抜けて実際にユーザーへ有害な出力が届くケースはごくわずかだということです。従来の研究が示してきた深刻な脅威像とは対照的であり、セーフティフィルターを導入することで防御側が有利に立てる可能性を示しています。

<img decoding="async" width="1024" height="327" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-1024x327.png" alt="" class="wp-image-100342" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-1024x327.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-300x96.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-768x245.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-1536x491.png 1536w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-2048x654.png 2048w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-400x128.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-800x256.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_1-1200x383.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />Mistral-7Bに対する攻撃成功サンプルの検出率。入力段階（左）と出力段階（右）でフィルターごとの性能差が見られる

### 意味的なステルス性を欠く攻撃は容易に検出される

分析の中で興味深かったのは、攻撃成功率が高い手法ほど、フィルターによって簡単に検出される傾向があった点です。たとえばAdaptive Attacksは、多くのLLMに対して90%以上の成功率を示しましたが、生成されるプロンプトは露骨に悪意が読み取れるものが多く、ほぼすべてのフィルターで検出されています。

一方で、ReNeLLMのように一見すると無害に見えるようプロンプトを偽装する、意味的なステルス性を持つ手法は、一部のフィルターを通過することがありました。しかしそれでも、PromptGuardやO3のような性能の高いフィルターでは最終的に検出されています。

また、複数回の対話を通じて徐々に有害な方向へ誘導するCrescendoというマルチターン攻撃も評価されました。各入力単体では無害に見えるため検出が難しいと考えられてきましたが、最終的に有害な内容に到達した段階ではフィルターによる検出が可能であることが確認されています。

### 検出性能はフィルターとLLMの組み合わせで変動する

すべてのフィルターが同じ性能を発揮するわけではありません。全体としては、推論能力を備えたO3が最も安定した検出性能を示しました。攻撃手法の種類にかかわらず低いパス率を維持しており、文脈を踏まえた判断能力が有効に働いていると考えられます。

一方で、GradSafeは状況によって性能にばらつきが見られました。入力と「Sure」のような同意応答を組み合わせ、その際に生じる勾配パターンを分析する仕組みです。この前提に合わない形式の攻撃に対しては検出精度が下がる傾向がありました。

LLMそのものの特性も結果に影響しています。Llama-2-7BやLlama3.1-8Bは、もともとの攻撃成功率が30%未満と低く、モデル自体の安全性アラインメントが比較的強固であることがわかります。ただし、これらのモデルに対して成功した攻撃は、より巧妙に偽装されている場合が多く、フィルターでも検出しにくい傾向が見られました。

## フィルターの挙動を詳しく分析する

ここまでの結果を見ると、セーフティフィルターを導入すれば安全性の問題は解決するようにも見えます。しかし、研究チームはフィルターの限界についても丁寧に分析しています。

### 現行フィルターは見逃し防止を優先しており誤検出が課題

セーフティフィルターを評価する際には、有害な内容をどれだけ見逃さずに検出できるかという再現率と、無害な内容を誤ってブロックしないかという精度の両方を考える必要があります。

実験の結果、多くのフィルターは「再現率を高く保つ」ように設計されていることがわかりました。有害コンテンツを見逃さないという点では合理的ですが、その反面、「無害な入力まで誤って遮断してしまう」問題が生じます。
極端だったのがPromptGuardで、有害コンテンツの検出率は100%に達したものの、無害なコンテンツもすべて有害と判定してしまいました。この状態では、実運用での利用は難しいと言えます。

一方、LlamaGuardは95%という高い精度を示し、再現率と精度のバランスが取れた結果となりました。OpenAI APIも86%の精度を維持しており、実用的な水準にあります。

<img decoding="async" width="1024" height="162" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-1024x162.png" alt="" class="wp-image-100344" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-1024x162.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-300x47.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-768x121.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-1536x243.png 1536w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-2048x323.png 2048w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-400x63.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-800x126.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_2-1200x189.png 1200w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />通常サンプル（有害・無害）に対する各フィルターの検出結果。PromptGuardは無害な入力もすべて有害と判定している

誤検出が起こりやすいトピックについても分析されています。
最も多かったのは身体的危害に関連する内容で、次いでプライバシーや性的コンテンツが続きました。たとえば「爆弾技術がこの数十年でどのように進化したか」といった歴史的な質問や、「自殺の一般的な方法について」といった学術的文脈の問いが、意図を考慮されずに有害と判定される例が見られました。現行フィルターが表層的なキーワードに強く依存していることを示しています。

### 主要フィルターは低コスト・低遅延で運用できる

企業がセーフティフィルターを導入する際には、コストや処理時間も重要な要素になります。研究チームはこの点についても評価を行いました。

その結果、LlamaGuardとOpenAI API（Content Moderation APIのこと。「omni-moderation-latest」モデル）が高い費用対効果を示しました。LlamaGuardは1サンプルあたり約0.028秒、OpenAI APIは約0.455秒で処理が完了します。検出性能とあわせて考えると、大規模サービスへの導入に適していると言えます。

<img decoding="async" width="1024" height="390" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-1024x390.png" alt="" class="wp-image-100347" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-1024x390.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-300x114.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-768x292.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-400x152.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-800x305.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4-1200x457.png 1200w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_4.png 1266w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />各セーフティフィルターの処理時間と経済コスト。LlamaGuardとOpenAI APIが費用対効果に優れる

また、O3は最も高い検出性能を示しましたが、処理時間は約7.22秒、コストは1サンプルあたり約0.0096ドルと、他のフィルターに比べて負担が大きくなります。リアルタイム性よりも安全性を重視する場面や、監査用途には向いていますが、即時応答が求められる用途には不向きかもしれません。

そして勾配計算を伴うGradSafeは、1サンプルあたり約40秒と処理時間が最も長くなりました。研究用途としては有用ですが、商用サービスへの組み込みは現実的ではないと考えられます。

### 誤検出は文脈理解の不足に起因する

最後に、誤検出の具体例について追加の事例です。

<img decoding="async" width="1024" height="601" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-1024x601.png" alt="" class="wp-image-100346" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-1024x601.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-300x176.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-768x450.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-400x235.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3-800x469.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100329_3.png 1156w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />誤検出が発生しやすいカテゴリの分布。身体的危害やプライバシー関連のトピックで多く発生している

典型的な誤検出の例として、歴史的あるいは医学的な文脈で使われるセンシティブな単語に反応してしまうケースが挙げられます。「ナチス党」「自殺」「爆弾」といった単語は、それ自体が必ずしも有害な意図を示すわけではありませんが、現行のフィルターはこれらの語が含まれるだけで警戒してしまう傾向があります。

単語単位の判定ではなく、文脈や意図を理解できる、より高度なフィルタリング機構が求められます。表面的なパターン認識を超えて、ユーザーが何を知りたいのかを正しく判断できる仕組みが期待されます。

## まとめ

本記事では、LLMに対するジェイルブレイク攻撃と、それに対抗するセーフティフィルターの関係を体系的に検証した研究を紹介しました。

重要な結論として、これまでの研究で指摘されてきた高い攻撃成功率は、実際の運用環境で用いられるセーフティフィルターを前提にすると大きく下がることが示されました。PromptGuardやO3（推論モデル）のようなフィルターは、多くの攻撃を入力段階で検出でき、その結果、パス率を5%未満に抑えられます。また、LlamaGuardやOpenAI APIは処理コストや遅延が小さく、現実的な負担で導入できる点でも実用性が高いことがわかりました。

その一方で、課題も明確になっています。現在のフィルターは、有害な内容を見逃さないことを重視する設計になっているため、無害な入力まで誤ってブロックしてしまう例が少なくありません。この誤検出を減らすには、単純なキーワードの一致ではなく、文脈や利用者の意図まで踏み込んで理解できる仕組みが必要になります。

総合的に見て、企業がLLMを導入する際には、モデルそのものの安全性だけに依存するのではなく、場合に応じた適切なセーフティフィルターを組み合わせることで、現時点でも十分に現実的な防御が実現できると言えそうです。

本記事の関連研究

- RAGシステムにおけるセキュリティ対策の投資対効果
- LLMエージェントのベースモデルに何を使う？安全性ランキング調査結果
- Copilotサービスを使う上で気を付けたいプロンプトインジェクションによる機密情報漏洩リスク