---
source_url: https://ai-data-base.com/archives/64708
captured_at: 2026-01-18T13:23:39.044Z
title: "小さなLLMを多数組み合わせることで、単一の巨大モデルに匹敵する可能性 - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T21:08:38+09:00
conversion_method: Readability+Turndown
file_hash: 79a9951630a2ad56
---

大規模言語モデル（LLM）のパフォーマンスを向上させる極めてシンプルな方法を発見したと報告されています。その方法とは、複数のエージェントを生成し、それらの結果を投票によって集計するというものです。既存の複雑なLLM強化手法とは異なる考え方です。

実験では、タスクの難易度と本方法論の効果の大きさが相関関係にあることが示されました。そして、エージェント数のスケーリング則（増やせば増やすほど性能が向上する法則）の可能性について詳細に調べられています。

![](https://ai-data-base.com/wp-content/uploads/2024/04/AIDB_64708_thum_3-1024x576.png)

**参照論文情報**

*   タイトル：More Agents Is All You Need
*   著者：Junyou Li, Qin Zhang, Yangbin Yu, Qiang Fu, Deheng Ye
*   所属：Tencent Inc.

**本記事の関連研究**：

関連研究：

*   [多様な役割のAIエージェント達に協力してソフトウェアを開発してもらう『ChatDev』登場。論文内容＆使い方を解説](https://ai-data-base.com/archives/54863)
*   [大規模言語モデル同士に上手く協力してソフトウェア開発をしてもらうフレームワーク「MetaGPT」](https://ai-data-base.com/archives/54189)
*   [1.1Bパラメータの小さなモデルを巨大データ（約3兆トークン）で訓練したモデル『TinyLlama』が、比較的優秀な性能を発揮](https://ai-data-base.com/archives/61914)
*   [既存のLLMを融合させて強力なモデルを作る手法「知識融合」](https://ai-data-base.com/archives/63153)

## 背景

LLMは、単一のモデルで複雑な問題に対応する能力には限界があるとされています。そこで研究者たちは複数のLLMを組み合わせる手法（アンサンブル手法）や、LLMエージェント同士が連携するフレームワークの開発に注力しています。例えば複数のLLMエージェントを用いた議論形式も提案されており、計算能力などの向上が示されています。

複数エージェントのフレームワークでは、エージェントの数を増やすことで性能が向上する傾向にあることも明らかになっています。とはいえ、その普遍性はまだ十分には検証されていません。そこで研究者らは、LLMエージェントのスケーリング特性に焦点を当て、この現象が一般に成り立つかどうかを調査することにしました。

研究者らは、「サンプリング＆投票」というシンプルな手法を提案しています。タスクの質問文をLLMまたはLLMエージェントの連携フレームワークに繰り返し入力し、複数の出力を生成し、多数決によって最終的な結果を決定する手法です。既存の複雑な手法とは全く独立する方法かつ、既存の複雑な手法の性能をさらに向上させる可能性を秘めています。

また、教師あり学習を必要とせず、追加の訓練データも必要としません。

## 方法論

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.