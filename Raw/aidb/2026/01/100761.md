---
source_url: https://ai-data-base.com/archives/100761
captured_at: 2026-01-18T18:29:03.028145
title: "RAG構築時に知っておくべきLLMの情報源バイアスと対策法"
publish_date: 2026.01.14
tags: ["分析", "LLM", "RAG", "手法", "426 実証", "137 分析", "54 サーベイ", "37 ベンチマーク・リソース", "22 テクニカルレポート", "15 ポジション", "8 LLM", "659 プロンプト技術", "156 エージェント", "128 コーディング", "56 RAG", "50 安全性", "39 ペルソナ・シミュレーション", "36 オープンソース", "25 マルチモーダル", "23 画像認識", "20 セキュリティ", "16 ハルシネーション", "16 ファインチューニング", "16 画像生成", "9 音声", "8 医療・ヘルスケア", "33 政治・社会", "29 エンタメ・アート", "23 金融・経済", "10 SE", "9 教育・キャリア", "9 製造・デザイン", "9 ロボット", "6"]
conversion_method: browser_subagent_v1
is_premium: unknown
---

本記事では、RAGにおいてLLMがどの情報源を信頼しやすいのかを調べた研究を紹介します。

RAGは、検索などによって外部から集めた情報をもとに、LLMが回答を生成する仕組みです。ただし、検索結果に含まれる複数の情報が互いに食い違っている場合、LLMがどの情報を採用するのかは明確ではありません。

![](https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-1024x576.jpg)

<img fetchpriority="high" decoding="async" width="1024" height="576" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-1024x576.jpg" alt="" class="wp-image-100766" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-1024x576.jpg 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-300x169.jpg 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-768x432.jpg 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-1536x864.jpg 1536w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-400x225.jpg 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-800x450.jpg 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761-1200x675.jpg 1200w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761.jpg 1600w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />

本記事の関連研究

- [RAGシステムにおけるセキュリティ対策の投資対効果](https://ai-data-base.com/archives/100063)
- [RAGで取得すべき情報はLLMごとの「データの有用性」で異なる](https://ai-data-base.com/archives/89169)
- [LLM自体の性能が飛躍的に向上した今、RAGに求められることとは](https://ai-data-base.com/archives/80093)

## 背景

LLMは、膨大なテキストデータを使って事前に学習されています。ただし、学習時に得た知識だけをもとに回答を生成すると、事実とは異なる内容をもっともらしく出力してしまう場合があります。このような現象は、ハルシネーションと呼ばれています。

この問題を抑える方法として、RAG（検索拡張生成）という手法が広く利用されています。RAGでは、ユーザーの質問に関連する情報を外部のデータベースやウェブから検索し、その結果を文脈としてLLMに与えます。外部情報を参照させることで、回答の正確さを高める狙いがあります。

一方で、RAGにも課題があります。検索によって得られた複数の情報が、互いに食い違っている場合です。たとえば、ある人物の経歴について、情報源Aと情報源Bで内容が異なっているとき、LLMはどちらの記述を選ぶのでしょうか。

これまでの研究から、LLMは質問との関連性が高い情報や、何度も登場する情報を優先しやすいことが分かっています。しかし、情報の内容ではなく、発信者や媒体といった情報源そのものが判断に与える影響については、ほとんど検討されてきませんでした。

そこで本記事では、LLMが情報源の種類に応じてどのような選好を示すのか、さらにその選好がどのような条件で崩れやすいのかを明らかにした研究を紹介します。

ここから限定コンテンツ

### 忙しい人向けに、重要なポイント5選

1. LLMは情報源の種類に明確な選好を持ち、政府、新聞、個人、SNSの順に信頼する階層構造が存在する
2. 情報源が明記されている情報は、出典のない情報よりも優先される傾向がある
3. 新聞の発行部数やSNSのフォロワー数が多いほど、その情報源からの情報が採用されやすくなる
4. 信頼性の低い情報源からの情報でも、単に繰り返し提示するだけでLLMの選好が逆転してしまう
5. ファインチューニングによる緩和手法により、繰り返しバイアスを最大99.8%削減しつつ元の選好を維持できる

参照文献情報

- タイトル：Whose Facts Win? LLM Source Preferences under Knowledge Conflicts
- URL：https://arxiv.org/abs/2601.03746
- 著者：Jakob Schuster, Vagrant Gautam, Katja Markert
- 所属：Heidelberg University

## 研究の進め方

LLMが異なる情報源から提示された矛盾する情報をどのように扱うのかを調べるため、合成データを用いた実験が行われています。

<img decoding="async" width="1024" height="384" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-1024x384.png" alt="" class="wp-image-100783" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-1024x384.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-300x113.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-768x288.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-400x150.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-800x300.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2-1200x450.png 1200w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_2.png 1362w" sizes="(max-width: 1024px) 100vw, 1024px" data-eio="l" />矛盾する情報に情報源を付与し、LLMの回答確率の変化を測定する実験の流れ

### 架空のエンティティを使った矛盾情報ペアの作成

最初に、知識の矛盾を人工的に再現するためのデータセットが構築されています。使用されているのは、NeoQAと呼ばれる既存のデータセットです。人物、建物、イベント、場所、組織、製品、芸術作品という7種類の架空エンティティが含まれており、それぞれに生年月日や本社所在地など、合計38種類の属性が設定されています。

本研究では、NeoQAに含まれる各エンティティについて、1つの属性値だけを変更した反事実バージョンが作成されています。たとえば、ある人物の国籍が「Evendese」と設定されている場合、国籍を「Altoranian」に変更した別バージョンが用意されます。数値属性については元の値からプラスマイナス20パーセントの範囲で調整され、カテゴリ属性についてはLLMを用いて現実的な代替値が生成されています。

その結果、373のエンティティから1,903個の反事実属性値が作られ、合計7,440組の矛盾情報ペアが構築されています。

### 4種類の架空の情報源

次に、情報の出所として用いる架空の情報源が4種類用意されています。

情報源の種類内容新聞米国の実在する新聞名から頻出する命名パターンを抽出し、「The 地名 Herald」のようなテンプレートを作成。テンプレートにNeoQAの架空の地名を当てはめることで、「The Hearthview District Daily Sun」といった架空の新聞名を生成。政府機関LLMを用いて「Civil Registry of 地名」や「Social Security Administration 地名」といった名称テンプレートを作成し、架空の地名と組み合わせて政府機関名を生成。SNSユーザー「@GrantedMortal7505」のように、@記号の後に形容詞、名詞、4桁の数字を組み合わせたユーザー名を生成。ユーザー名の構成方法は、Redditのユーザー名提案アルゴリズムを参考にしている。個人名米国国勢調査局と社会保障局のデータをもとに、1945年から2007年の間によく使われていた名前を抽出。男女が均等になるようにサンプリングし、実在する著名人と同一になる名前の組み合わせは除外。

### 13種類のオープンモデルを多角的に評価

評価対象として、4つのモデルファミリーに属する13種類のオープンウェイトモデルが選ばれています。具体的には、QWEN2.5（7B、14B、32B、72B）、OLMo-2（7B、13B、32B）、LLaMA-3.2（3B）、LLaMA-3.1（8B、70B）、GEMMA-3（4B、12B、27B）が使用されています。

### 情報源の影響を測定する評価指標

実験では、互いに矛盾する2つの情報をMarkdown形式のテーブルとしてLLMに提示し、多肢選択式の質問に回答させる方法が採用されています。テーブル形式が使われている理由は、文章表現や文体の違いによる影響を抑えるためです。

評価の中心となるのが、「情報源選好指標」と呼ばれる指標です。まず、情報源を明示しない状態でLLMに回答させ、選択肢Aと選択肢Bが選ばれる確率を記録します。その後、同じ矛盾情報ペアに情報源を付与した状態で再度回答させます。情報源の有無による確率の変化を計算することで、情報源がLLMの判断に与えた影響を定量的に測定しています。

さらに、LLMには選択肢の提示順によって回答が偏る位置バイアスが存在することが知られています。この影響を打ち消すため、すべてのデータについてテーブルの順序を入れ替えた2種類のプロンプトが作成されています。統計的な検定にはブートストラップ法が用いられ、13モデルのうち10モデル以上で有意な結果が確認された項目のみが報告されています。

## 実験結果

実験を通して見えてきたLLMの「情報源に対する考え方」を整理します。

### 政府、新聞、個人、SNSの順で信頼される明確な階層構造

最初に、政府機関、新聞、個人、SNSユーザーという4種類の情報源について、LLMがどの順番で信頼するかが調べられました。

情報源が書かれている情報と、情報源が書かれていない情報を比べると、13種類すべてのモデルが、情報源が明示された情報を選びやすいことが分かりました。発言の中身だけでなく、発信元が示されている点も、LLMにとっては重要な判断材料になっています。

次に、異なる情報源同士を直接比べる実験が行われました。その結果、すべてのモデルで、情報源順位に一貫したルールが見られました。政府機関が新聞より優先され、新聞が個人より優先される場合、政府機関が個人よりも優先されるという関係が崩れなかったのです。

<img decoding="async" width="871" height="826" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3.png" alt="" class="wp-image-100785" style="aspect-ratio:1.054489819936448;width:618px;height:auto" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3.png 871w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3-300x285.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3-768x728.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3-400x379.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_3-800x759.png 800w" sizes="(max-width: 871px) 100vw, 871px" data-eio="l" />すべてのモデルで政府 > 新聞 > 個人という一貫した階層構造が確認された

この実験から、政府機関が最も信頼され、その次に新聞、続いて個人とSNSユーザーが並ぶという構造が確認されています。13モデル中11モデルでは、政府機関や新聞といった制度的な情報源が、個人やSNSユーザーよりも上位に置かれました。モデルの種類やサイズが違っても、似た判断が行われていることが分かります。

<img decoding="async" width="719" height="348" loading="lazy" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_1.png" alt="" class="wp-image-100782" style="width:575px;height:auto" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_1.png 719w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_1-300x145.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_1-400x194.png 400w" sizes="auto, (max-width: 719px) 100vw, 719px" data-eio="l" />LLMは政府、新聞、個人、SNSの順に情報を信頼するが、繰り返しによって逆転する

### 発行部数やフォロワー数が多いほど信頼される

同じ種類の情報源の中でも、条件によって信頼のされ方が変わるかどうかも調べられています。

新聞については、発行部数の違いが比べられました。発行部数が少ない新聞と多い新聞を比べると、すべてのモデルが、発行部数の多い新聞からの情報を選ぶ傾向を示しています。

SNSユーザーについては、フォロワー数の違いが検証されました。フォロワー数が少ないアカウントと多いアカウントを比べた結果、こちらもすべてのモデルが、フォロワー数の多いアカウントを優先しています。

これらの結果から、LLMは情報源の人気や影響力を、信頼できそうかどうかを判断する手がかりとして使っていることがうかがえます。

### 学術称号や年齢の影響は限定的

情報源に関する個人的な属性についても分析が行われています。

新聞の地域性については、扱われているエンティティと同じ地域にある新聞が、少しだけ優先される傾向が見られました。ただし、その差は大きくありません。

個人を情報源とした場合には、学術称号、性別、年齢が調べられています。博士号などの学術称号を持つ人物からの情報は、わずかに選ばれやすい傾向がありました。また、女性からの情報も、わずかに優先される傾向が見られています。年齢については、若年層が最も選ばれにくく、中年層と高齢層の順位はモデルごとに違いがありました。

SNSユーザーの名前の付け方については、名前をアンダースコアでつないだ形式と、形容詞と名詞を組み合わせた形式が比べられましたが、どちらが好まれるかはモデルによって分かれています。

全体を見ると、学術称号や年齢といった要素の影響は、情報源の種類や発行部数、フォロワー数と比べると小さいことが分かります。

### プロンプトで尋ねた選好と実際の行動は概ね一致するが例外もある

最後に、矛盾した情報を処理させるのではなく、どちらの情報源がより信頼できるかを直接LLMに聞いた場合の答えと、実際の選択行動がどの程度一致するかが調べられました。

195件の比較のうち、139件では、こうした直接的な質問で示された選好が、実際の行動と同じ方向で、より強く表れています。一方で、38件では、直接的な質問の結果と行動が逆になるケースも確認されました。

直接的な質問による評価では、モデル間で判断がそろいやすくなる傾向も見られます。ただし、直接的な質問は極端な判断を引き出しやすい一方で、実際の利用場面に近い挙動を反映しているのは、行動ベースの評価だといえます。

## 情報源の信頼性と繰り返しバイアスの相互作用

実際のRAGでは、同じ内容の情報が複数回出てくる場面がよくあります。そこで、情報源の信頼性と、同じ情報が何度も出てくることが、どのように組み合わさって影響するのか調べられています。

### 繰り返し提示するだけで信頼性の階層構造が逆転する

この研究では、政府機関とSNSユーザーという、信頼性が大きく異なる2種類の情報源を使って、3つの条件で実験が行われました。

1つ目の条件では、3つの表が提示されます。2つの表には、別々のSNSユーザーからの同じ内容が書かれ、残り1つの表には、政府機関からの矛盾する内容が書かれています。この場合、すべてのモデルが、政府機関ではなくSNSユーザーの情報を選びました。

2つ目の条件では、2人のSNSユーザーを1つの表にまとめて表示し、表そのものは1つだけにしました。この場合、モデルは政府機関からの情報を優先し続けました。

3つ目の条件では、3つの表が提示されますが、2つの表は同じSNSユーザーによる同じ内容です。新しい情報源は増えていません。この場合でも、ほとんどすべてのモデルが、繰り返し表示されたSNSユーザーの情報を選びました。

<img decoding="async" width="1024" height="564" loading="lazy" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-1024x564.png" alt="" class="wp-image-100786" style="width:678px;height:auto" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-1024x564.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-300x165.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-768x423.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-400x220.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4-800x441.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_4.png 1095w" sizes="auto, (max-width: 1024px) 100vw, 1024px" data-eio="l" />同じ情報を繰り返すだけで、SNSユーザーが政府機関より優先されるようになる

以上から分かるのは、LLMが多数派に従っているように見える場面でも、実際には「何人が言っているか」よりも「何回見たか」が強く効いているという点です。同じ情報が繰り返しされるだけで、信頼性の高い情報源よりも優先されてしまいます。

### 情報源がない場合でも繰り返しの効果は持続する

情報源をまったく示さず、同じ内容だけを繰り返した場合でも、同様の傾向が確認されています。出典が書かれていない情報であっても、何度も提示されると、政府機関や新聞からの情報より優先されることがあります。

人間に見られる錯覚的真実効果とよく似ています。錯覚的真実効果とは、内容が正しいかどうかに関係なく、何度も見た情報を正しいと感じやすくなる現象です。人間で確認されているこの傾向が、LLMにも表れていると考えられます。

### 信頼性を考慮するよう指示しても完全には解消されない

繰り返しによる影響を弱めるために、プロンプトで信頼性を意識させる工夫も試されています。回答を選ぶ前に、情報源を整理し、信頼できそうかどうかを考えるよう指示が追加されました。

この工夫によって、政府機関や新聞を優先する傾向は強まり、繰り返しの影響もある程度は抑えられました。

ただし、繰り返しがない場合と同じ判断に戻るところまでは至っていませんでした。文章で注意を促すだけでは、繰り返しバイアスを完全に防ぐのは難しいということです。

## 繰り返しバイアスの緩和手法

文章で注意を促すだけでは、繰り返しによる偏りを十分に抑えられないことが分かりました。そこで研究者らは、もう一歩踏み込んだ方法として、ファインチューニングによる対策を試しています。

使われたは、教師モデルの判断のしかたを、生徒モデルに少しずつ学ばせる知識蒸留という考え方です。ポイントは、同じ矛盾情報に対して、情報が何度も出てくる場合でも、最初と同じ判断を保てるようにすることです。繰り返しがない状態での判断を基準にして、繰り返しがある状態でも判断がぶれないように学習が行われます。

実験では、GEMMA-3-4Bという比較的小さなモデルが使われ、LoRAによる軽量なファインチューニングが行われました。限られた数のデータだけを使い、訓練に含まれていない情報源やエンティティに対しても評価が行われています。

その結果、繰り返しによって起きていた判断の逆転は大きく減りました。政府機関と情報源なしの比較では、繰り返しの影響はほぼ見られなくなり、政府機関を優先する元の傾向もほとんど保たれています。政府機関とSNSユーザーの比較でも、繰り返しの影響ははっきりと弱まりました。

つまりファインチューニングを取り入れることで、同じ情報が何度出てきても振り回されにくい、安定した判断ができるLLMに近づけることが分かります。

<img decoding="async" width="1024" height="625" loading="lazy" src="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-1024x625.png" alt="" class="wp-image-100788" style="aspect-ratio:1.6384249664756796;width:726px;height:auto" srcset="https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-1024x625.png 1024w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-300x183.png 300w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-768x469.png 768w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-400x244.png 400w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5-800x488.png 800w, https://ai-data-base.com/wp-content/uploads/2026/01/AIDB_100761_5.png 1085w" sizes="auto, (max-width: 1024px) 100vw, 1024px" data-eio="l" />ファインチューニングにより繰り返しバイアスが大幅に削減され、元の選好が維持された

## まとめ

本記事では、RAGにおけるLLMの情報源選好と、その脆弱性および緩和手法に関する研究を取り上げました。

実験の結果、LLMは政府機関、新聞、個人、SNSユーザーの順に情報を信頼するという明確な階層構造を持つことが明らかになっています。また、発行部数やフォロワー数といった人気指標も信頼性の判断に影響を与えることがわかりました。

一方で、信頼性の低い情報源からの情報でも、単に繰り返し提示するだけでLLMの選好が逆転してしまうという意外な脆弱性も発見されています。この繰り返しバイアスはプロンプトによる対策だけでは解消できませんが、ファインチューニングと信頼性プロンプティングを組み合わせることで、最大99.8%削減できることが示されました。

RAGシステムを構築・運用する際には、検索結果に含まれる情報がLLMの判断を歪める可能性があることを念頭に置く必要があるといえます。

本記事の関連研究

- [RAGシステムにおけるセキュリティ対策の投資対効果](https://ai-data-base.com/archives/100063)
- [RAGで取得すべき情報はLLMごとの「データの有用性」で異なる](https://ai-data-base.com/archives/89169)
- [LLM自体の性能が飛躍的に向上した今、RAGに求められることとは](https://ai-data-base.com/archives/80093)