---
source_url: https://ai-data-base.com/archives/63749
captured_at: 2026-01-18T13:22:50.612Z
title: "複数LLMに議論させ、「回答に自信がないときは発言を控えさせ」て応答品質を向上する方法 - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T21:08:42+09:00
conversion_method: Readability+Turndown
file_hash: acec7eebc03b4542
---

知識は常に変化するため、どんなにLLMの知識を拡張しようと、欠落や古くなった情報が残ってしまう可能性があります。

既存手法は自己分析能力に欠け、データセットへの過度の依存があることから、今回ワシントン大学やUCバークレーなどの研究者らはLLM同士が互いの知識を検証する手法を提案しました。

3つのLLM、4つの質問応答タスクで実施した実験により、ベースラインに対して最大19.3%の精度向上を確認しました。

![](https://ai-data-base.com/wp-content/uploads/2024/02/AIDB_63749-1024x576.jpg)

**参照論文情報**

*   タイトル：Don’t Hallucinate, Abstain: Identifying LLM Knowledge Gaps via Multi-LLM Collaboration
*   著者：Shangbin Feng, Weijia Shi, Yike Wang, Wenxuan Ding, Vidhisha Balachandran, Yulia Tsvetkov
*   所属：ワシントン大学, カリフォルニア大学バークレー校, 香港科技大学, カーネギーメロン大学

**本記事の関連研究**：

*   [LLMの内部状態を観察することで「出力がハルシネーションか否かを判別する」手法『LLMファクトスコープ』](https://ai-data-base.com/archives/61651)
*   [LLMの誤り（ハルシネーション）発生原因と、「創造性と事実性のバランス」などの対策ロードマップ](https://ai-data-base.com/archives/58767)
*   [LLMの出力から誤り（ハルシネーション）を減らす新手法『CoVe（Chain-of-Verification）』と実行プロンプト](https://ai-data-base.com/archives/55711)
*   [LLMに「自信の度合いに応じて説明のニュアンスを変更させる」ことで人間が過度に信頼するのを防ぐ](https://ai-data-base.com/archives/63482)
*   [LLMに自身のハルシネーション（幻覚）を「自覚」させ、減らす方法](https://ai-data-base.com/archives/55232)

## 背景

LLMは膨大な知識を保有していますが、知識が欠落していたりそもそも不正確な場合、誤った回答を生成してしまう恐れがあります。そんな事態を防ぐため、信頼度の低い回答は控える機能が必要だという見方があります。

そこで課題は、LLMが自身の知識不足をどのように特定できるのか、ということです。

既存の手法は、多くの場合、外部データや、LLMが自身を客観的に評価できるという仮定に依存しており、必ずしも正確な判断ができない場合があります。  
中でも、「単一のLLMが自分自身の回答を客観的に見直して誤りを正せる」といった考えは過度な期待に基づくものだと研究者らは指摘しています。

そこで今回、複数のLLMによって回答を客観視し、信頼性が低いときには回答を控えさせる手法が提案されています。誤解を招く情報生成を避けることで、システムの実用性を上げることにつながる手法です。

以下で詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.