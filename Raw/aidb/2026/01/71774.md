---
source_url: https://ai-data-base.com/archives/71774
captured_at: 2026-01-18T17:58:24.971962
title: "RAGにおいて長文を検索することで一気に効率を上げるアプローチ"
publish_date: 2026.01.14
tags: ["手法", "LLM", "RAG"]
conversion_method: browser_subagent_v1
is_premium: unknown
---

RAGには、膨大なデータの中から関連性のある短い文章を探し出すのが難しいという課題があります。研究者らは、より大きな「塊」で情報を探すことで、検索の効率を劇的に向上させる手法「LongRAG」を考案しました。

## LongRAGの考え方
LongRAGでは、約4,000トークンからなる長い文章を検索単位として採用します。Wikipedia等の文書をグループ化することで、検索対象のデータ量を大幅に削減（2,200万件から60万件に）し、検索システムの負担を軽減します。

## 実験結果
NQデータセットにおいて、トップ1の回答再現率が52.24%から71.69%に改善されました。最適な入力トークン数は約30,000トークンであることが示唆されています。

## まとめ
LongRAGは特に複雑な質問や多段階の推論を必要とする問題に対して優れた性能を発揮することが実証されました。