---
source_url: https://ai-data-base.com/archives/74922
captured_at: 2026-01-18T13:30:07.136Z
title: "RAGで検索文書の要約を活用したクエリ書き換えが検索精度を大幅に向上させる AWS報告 - AIDB"
category: "unknown"
is_premium: false
publish_date: 2025-03-08T19:25:06+09:00
conversion_method: Readability+Turndown
file_hash: f64de8c806923132
---

本記事では、LLMを活用したRAGシステムにおける新しいアプローチを紹介します。

より効果的な情報検索と回答生成を実現するために、質問回答ペアをLLMで生成して参照する仕組みです。

モデルの微調整不要でコスト効率も高い手法として期待されています。

![](https://ai-data-base.com/wp-content/uploads/2024/08/AIDB_74922-1024x576.jpg)

**参照論文情報**

*   タイトル：Meta Knowledge for Retrieval Augmented Large Language Models
*   著者：Laurent Mombaerts, Terry Ding, Adi Banerjee, Florian Felice, Jonathan Taws, Tarik Borogovac
*   所属：AWS

**本記事の関連研究**

*   [RAGとLong-Contextの比較、そしてハイブリッドで活用する新しい方法](https://ai-data-base.com/archives/73468)
*   [100個の事例を分析して明らかになったLLM-RAGアプリケーション「19の欠陥パターン」](https://ai-data-base.com/archives/73120)
*   [NVIDIAが教えるRAGチャットボット実装の重要ポイント](https://ai-data-base.com/archives/72680)
*   [RAGシステムの最適な構築を探る](https://ai-data-base.com/archives/72121)

## 背景

検索結果を用いてLLMの出力を補強する手法である「検索拡張生成（RAG）」が、広く使用されるようになりつつあります。LLMに最新かつ正確な情報を提供することで、モデルの誤った情報生成を減らし、タスクに適した回答を生成する能力を向上させるアプローチです。

RAGシステムの構築には依然として難しい課題が残されています。大規模かつ多様な文書データから情報を効果的に探し出すのは至難の業であるという点です。通常のRAGの仕組みとしては、文書を単純に分割してチャンクとして扱います。しかしこれでは文脈の意味のつながりが失われる恐れがあります。また、そもそもユーザーの意図を正確に捉えて、適切な検索クエリを生成することも難しい課題の一つです。

これまでもRAGシステムを改善すべく様々なアプローチが提案されてきました。しかし、提案されてきた手法は往々にして、検索対象となる文書の特性を考慮できていないという問題がありました。

そこで今回研究者らは、従来の「検索してから読む」方式を拡張し、「”準備してから書き換えて”検索して読む」枠組みを考案しました。この方法を使うとユーザーの特性に合わせたクエリ拡張が可能となり、知識ベース全体にわたる深い情報検索が実現できることが分かりました。チャンキングに伴う情報損失を軽減し、ノイズフィルタリングの役割も果たします。

本手法は実用性の高いアプローチとして注目されています。検索の精度と再現率が向上し、最終的な回答の幅広さ、深さ、関連性、具体性が大幅に改善されることが実験で分かりました。

以下で詳しく紹介します。

プレミアム会員限定コンテンツです

閲覧には、アカウント作成後の決済が必要です。

*   全記事・論文コンテンツを無制限で閲覧可能
*   平日毎日更新、専門家による最新リサーチを配信

[プレミアム会員について](https://ai-data-base.com/premium-visitor)

Copyright © Parks, Inc. All rights reserved.