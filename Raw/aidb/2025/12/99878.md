---
source_url: https://ai-data-base.com/archives/99878
captured_at: 2026-01-18T20:44:53.272192
title: "2025年AIDBのXで最も読まれたAI論文を振り返る"
publish_date: 2025.12.27
tags: ["LLM", "エージェント", "ペルソナ・シミュレーション", "政治・社会", "手法\n426", "実証\n137", "分析\n54", "サーベイ\n37", "ベンチマーク・リソース\n22", "テクニカルレポート\n15", "ポジション\n8", "LLM\n659", "プロンプト技術\n156", "エージェント\n128", "コーディング\n56", "RAG\n50", "安全性\n39", "ペルソナ・シミュレーション\n36", "オープンソース\n25", "マルチモーダル\n23", "画像認識\n20", "セキュリティ\n16", "ハルシネーション\n16", "ファインチューニング\n16", "画像生成\n9", "音声\n8", "医療・ヘルスケア\n33", "政治・社会\n29", "エンタメ・アート\n23", "金融・経済\n10", "SE\n9", "教育・キャリア\n9", "製造・デザイン\n9", "ロボット\n6"]
conversion_method: browser_subagent_v1
is_premium: unknown
---

本企画では、AIDBのXで紹介した論文から、2025年に特に反響が大きかった研究内容をランキング形式でまとめ、「今年どんな話題が広がったのか」を振り返ります。

2025年は、巨大シミュレーションのように“試せる世界”を作る動きや、メンタル・教育・医療など“人の側”に近づく動きが同時に進んだ一年だったように見えます。
さらに、見えにくいリスクや運用上の工夫まで含めて注目される場面が増え、単純な性能比較だけでは語りきれない論点が前に出てきました。

## 第12位　10億人シミュレーションの社会モデル

```
Modeling Earth-Scale Human-Like Societies with One Billion Agents
https://doi.org/10.48550/arXiv.2506.12078
Haoxiang Guan, Jiyan He, Liyang Fan, Zhenzhen Ren, Shaobin He, Xin Yu, Yuan Chen, Shuxin Zheng, Tie-Yan Liu, Zhen Liu
（Zhongguancun Academy, Zhongguancun Institute of Artificial Intelligence, Shenzhen University, Shanghai University of Finance and Economics, Tsinghua University）
```

AIで10億人規模の人口をシミュレーションできるシステムを開発したとの報告。
一人ひとり異なる性格や背景を持つAIエージェントが10億人それぞれ実際の人間のように考えて行動します。

このシステムを使った仮想的な社会実験では、「社会的地位が高い人ほど他人を信頼しやすい」「教育レベルが高い人ほど親切に振る舞う」現象が観察されました。人数が増えるほど鮮明になったとのことです。

SNSで意見がどう広まるかを調べられ、インフルエンサー의意見が何億人もの考え方を大きく左右してしまうことも分かりました。
また、教育レベルの高い人は他人の意見を変えやすい一方で、自分の意見は変えにくいという特徴も明らかになりました。

こうした技術を応用すると、現実で確かめるのはリスクが高いこと（例えば政策など）を念入りに仮想実験できるようになるかもと期待されています。

### Xのポスト

### 関連記事

[個人の深い価値観にもとづく「その人らしい答え」をAIで再現する手法](https://ai-data-base.com/archives/90734)

## 第11位　対話で思い込みをほどくAI式の支援法

```
Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues
https://doi.org/10.48550/arXiv.2504.17238
Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang
```

人は落ち込んだり悩んだりすると「もう無理」「ぜんぶ失敗だった」などと極端な考えに走りがちです。
しかし適切に調整したAIと対話することで、バランスを取り戻して穏やかになれる可能性があるとのこと。

清華大学やハーバード大学などの研究者らによる報告です。

上述した極端な考えは”認知のゆがみ”と呼ばれるもので、本来は人間の専門家による心理療法が役立ちます。しかし専門家は不足している上に、メンタルの問題を他人に相談することをおっくうに感じる人は少なくありません。

そこで研究者らは、LLMベースの方法論を考案しました。単に考え方を矯正するのではなく、何ターンものやりとりを通して、徐々に考え方を変えてくれるそうです。
感情をケアしながら、どこに思い込みがあるのかを認識させてくれるとのことです。

この仕組みはPeppyという名前でウェブアプリケーション化されており、日本語でも対話することが可能です。

### Xのポスト

### 関連記事

[LLMをセラピストとして実行し、「認知の歪み」を診断させるためのプロンプト手法](https://ai-data-base.com/archives/56696)

## 第10位　LLMに潜む人間不信の正体

```
Measurement of LLM's Philosophies of Human Nature
https://doi.org/10.48550/arXiv.2504.02304
Minheng Ni, Ennan Wu, Zidong Gong, Zhengyuan Yang, Linjie Li, Chung-Ching Lin, Kevin Lin, Lijuan Wang, Wangmeng Zuo
（Hong Kong Polytechnic University, Harbin Institute of Technology, University of Bologna, Sichuan University, Microsoft）
```

Microsoftなどの研究者らによると、LLMには全体的に人間に対して不信感があるそうです。オープンソース・クローズドソース問わず共通している傾向であり、より賢いモデルほど不信感が強いとのこと。

さらに、訓練データの期間が最近になるほど不信感が増大する傾向もあると述べられています。

研究者らはこの状況を改善すべく、LLMの人間観を良いものに変える「メンタルループ学習」という根本的な訓練手法を開発しました。
こうでもしないと、単に「あなたはポジティブなAIです」のような明るいペルソナを設定するだけでは不信感は解消されないことが分かったためです（むしろ悪化することが判明）。

このメンタルループ学習を施すと、人間に対する信頼感が大幅に改善することが実験で明らかになりました。

### Xのポスト

### 関連記事

[開発企業や言語ごとに異なるLLMのイデオロギー、価値観や態度](https://ai-data-base.com/archives/77645)

## 第9位　大喜利を科学してLLMにチャレンジさせる

```
Oogiri-Master: Benchmarking Humor Understanding via Oogiri
https://doi.org/10.48550/arXiv.2512.21494
Soichiro Murakami, Hidetaka Kamigaito, Hiroya Takamura, Manabu Okumura
（CyberAgent, Nara Institute of Science and Technology, Institute of Science Tokyo）
```

サイバーエージェントとNAIST、東京科学大学のチームが、「大喜利」を科学的に分析し、得られた知見をプロンプトに組み込むことでGPT-5が人間を超える成績を出したそうです。

その知見とは、例えば、面白い回答は長々と説明するのではなく短いこと。そしてお題から離れた斬新な語彙を使うのではなく、お題の世界観にとどまりながら「視点を変える」ことが最も効果的などでした。

ただしClaude-Opus-4は同じ手法で逆に成績が下がりました。「短い方が面白い」といったヒントを文字通りに受け取りすぎて、極端に短い回答ばかり選んでしまったのです。
しかし、「迷ったときだけヒントを参照しろ」と指示すると、この問題が緩和されたとのこと。

昔からユーモア理論で重視されてきた「意外性」や「予測のしにくさ」といった要素は、実際はあまり効いていないことも示唆されました。

### Xのポスト

### 関連記事

[大喜利データセットでLLMをユーモアラスにチューニングする手法](https://ai-data-base.com/archives/60765)

## 第8位　9つの臓器AIで仮想人体を組む

```
Organ-Agents: Virtual Human Physiology Simulator via LLMs
https://doi.org/10.48550/arXiv.2508.14357
Rihao Chang, He Jiao, Weizhi Nie, Honglin Guo, Keliang Xie, Zhenhua Wu, Lina Zhao, Yunpeng Bai, Yongtao Ma, Lanjun Wang, Yuting Su, Xi Gao, Weijie Wang, Nicu Sebe, Bruno Lepri, Bingwei Sun
（Tianjin University, University of Trento, Tianjin Medical University General Hospital, Chest Hospital, Tianjin University, The Affiliated Suzhou Hospital of Nanjing Medical University, Fondazione Bruno Kessler）
```

人間の体を9つ（心臓、肺、腎臓など）に分けてそれぞれをAIが担当する、仮想人体のシステムを作成したと報告されています。

これを使い実際の患者さんの状態を12時間先まで予測できることが判明しています。
15人の集中治療の専門医が概ね高く評価するレベルに達したそうです。

また、「もしあの時違う治療をしていたらどうなっていたか」といった別のシナリオもシミュレーションします。
各臓器間の相互作用を連鎖反応まで表現できるとのことです。

こうした技術を使うことで、異なる治療選択肢の効果を事前に比較検討できるようになることが期待されています。

### Xのポスト

### 関連記事

[科学分野におけるLLM活用の発展まとめ](https://ai-data-base.com/archives/89070)

## 第7位　GPT-4.5とチューリングテスト報告

```
Large Language Models Pass the Turing Test
https://doi.org/10.48550/arXiv.2503.23674
Cameron R. Jones, Benjamin K. Bergen
UC San Diego
```

OpenAI社が提供するLLM「GPT-4.5」がチューリングテストに合格したとの発表です。カリフォルニア大学の研究チームによる報告。

人間の審査員が人間とAIを見分けようとした結果、73%の割合で人間ではなくGPT-4.5が”人間”と判定される結果だったと報告されています。
平たく表現するなら、人間よりも人間らしく振舞えるようになったことを示唆しています。

LLMが明確に高いスコアでチューリングテストに合格するのは初の事例とのことです。

同社がより安価に提供している「GPT-4o」は21%だったことから、人間らしさという基準で開発が大きく前進したことが分かります。

なお、”人間らしさ”のスコアが最も上昇する際の指示は、若く内向的でインターネット文化に詳しい人物のように振舞わせるときだったそうです。
LLaMa-3.1-405Bも同様のペルソナ指示で56%のスコアを達成しています。

チューリングテストは、審査員が人間と機械の両方と会話し、どちらが人間かを見分けられるかを試す方法で”人間らしさ”の評価が行われる試験です。

### Xのポスト

### 関連記事

[GPT-4、チューリングテストに一定の確率で成功](https://ai-data-base.com/archives/58540)

## 第6位　ポケモン対戦で推論力を試す

```
Large Language Models as Pokémon Battle Agents: Strategic Play and Content Generation
https://doi.org/10.48550/arXiv.2512.17308
Daksh Jain, Aarya Jain, Ashutosh Desai, Avyakt Verma, Ishan Bhanuka, Pratik Narang, Dhruv Kumar
（Birla Institute of Technology and Science, Pilani, India）
```

ポケモンバトルは、タイプ相性、ステータス、技の威力と命中率など、複数の要素を同時に考慮する必要があるため、LLMの推論能力を試すのに適している。とのこと。

実際に競わせたところ、Grok 4は圧倒的に強く、ほぼ完璧な勝率で平均6ターン以内にバトルを終わらせたそうです。
一方でClaude やDeepSeekは慎重な戦略を取り、20ターン以上かかる長期戦になる傾向がありました。

ただ、いずれにしても人間が感じる「適度な難易度の相手」として機能できることが分かりました。

ついでに、新しいポケモンの技を作らせるという不思議な実験も行われており、GPT-5 Miniは最も創造的で独創的な技を生み出し、Claudeは数値バランスの取れた実用的な技を作るのが得意でした。

### Xのポスト

### 関連記事

[LLMエージェントによって自然言語をゲーム理論モデルに変換する方法](https://ai-data-base.com/archives/81866)

## 第5位　立場で変わる生成AIの使われ方

```
The AI Gap: How Socioeconomic Status Affects Language Technology Interactions
https://doi.org/10.48550/arXiv.2505.12158
Elisa Bassignana, Amanda Cercas Curry, Dirk Hovy
（IT University of Copenhagen, Pioneer Centre for AI, CENTAI Institute, Bocconi University）
```

AIの使い方には、社会的な立場の違いがはっきりと現れているとの調査報告。

たとえば、収入が高く教育歴のある人たちは、AIを仕事や学習の効率化のために活用する傾向があります。
プログラミング、文章の添削、データ分析といった専門的な作業に使い、AIを「成果を上げるためのツール」として使っています。
また彼らは短く端的に指示を出し、無駄なく目的を達成しようとする傾向があります。

一方でそうでない人たちは、AIを雑談や日常的な相談相手のように使う傾向があるとのことです。
挨拶したり、料理のレシピを聞いたり、一般的な知識を尋ねたりと、身近なやりとりが中心。
AIを「便利な会話相手」として捉えているようです。

つまり、同じ技術を使っていても、立場によってその活用方法がまったく異なっています。
そして、この違いが格差をさらに広げてしまう可能性があると、研究チームは警鐘を鳴らしています。

社会的に有利な立場の人は AIを使ってさらに成果を伸ばし、そうでない人は日常を便利にする以上の使い方ができずに取り残されるかもしれないということです。

この「AI格差」は、将来のチャンスどころか、社会に参加する機会にすら影響する可能性があります。

これを受け、AIが誰にとっても公平に役立つ存在であるために、社会的背景の違いに配慮した設計が必要だと提言されています。

### Xのポスト

### 関連記事

[ChatGPTを使用する知的労働者のパフォーマンスは軒並み向上し、もとの成績が良くないほど顕著との調査結果](https://ai-data-base.com/archives/55470)

## 第4位　脳型アーキテクチャに引き続き注目

```
Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems
https://doi.org/10.48550/arXiv.2504.01990
Bang Liu, Xinfeng Li, Jiayi Zhang, Jinlin Wang, Tanjin He, Sirui Hong, Hongzhang Liu, Shaokun Zhang, Kaitao Song, Kunlun Zhu, Yuheng Cheng, Suyuchen Wang, Xiaoqiang Wang, Yuyu Luo, Haibo Jin, Peiyan Zhang, Ollie Liu, Jiaqi Chen, Huan Zhang, Zhaoyang Yu, Haochen Shi, Boyan Li, Dekun Wu, Fengwei Teng, Xiaojun Jia, Jiawei Xu, Jinyu Xiang, Yizhang Lin, Tianming Liu, Tongliang Liu, Yu Su, Huan Sun, Glen Berseth, Jianyun Nie, Ian Foster, Logan Ward, Qingyun Wu, Yu Gu, Mingchen Zhuge, Xiangru Tang, Haohan Wang, Jiaxuan You, Chi Wang, Jian Pei, Qiang Yang, Xiaoliang Qi, Chenglin Wu
（MetaGPT, Université de Montréal, Mila - Quebec AI Institute, Nanyang Technological University, Argonne National Laboratory, University of Sydney, Penn State University, Microsoft Research Asia, University of Illinois at Urbana-Champaign, The Hong Kong University of Science and Technology, University of Southern California, Yale University, Stanford University, University of Georgia, The Ohio State University, King Abdullah University of Science and Technology, Duke University, The Hong Kong Polytechnic University, Google DeepMind, Canada CIFAR AI Chair）
```

「AIエージェントを”総合的に知的な存在”にするには、脳のような構造が必要である」そんな主張を軸に、実に264ページにもおよぶ網羅的な論文が公開されました。

Microsoft、スタンフォード大学、Google DeepMindなど、世界20機関から46名の研究者らが名を連ねています。

人の脳のように
多様なパーツを組み合わせることで、LLMの能力をベースとしつつ、自律的かつ安全な知性を実現しよう。それこそが、AIと人間が共存する未来に向けた設計原則である、という立場です。

内部機能としては「認知」「記憶」「世界モデル（＝環境の変化を予測する力）」「報酬」「感情」「行動」などを組み込み、”自律的に学習する能力”や”他者と協力する能力”を備えた上で、安全性と道徳性の検証を前提とする構造が提案されています。

### Xのポスト

### 関連記事

[LLMの機能別「領域」はまるで脳のようであるとの仮説](https://ai-data-base.com/archives/78200)

## 第3位　250語作文から学びの伸びを読む

```
Large language models predict cognition and education close to or better than genomics or expert assessment
https://doi.org/10.1038/s44271-025-00274-x
Tobias Wolfram
（Faculty of Sociology, Bielefeld University, Bielefeld, Germany）

*nature communications psychology
```

11歳の子どもが書いた250語程度の短い作文をLLMで分析すると、その子の将来の学力や教育成果を予測できることを実験的に発見したとの報告です。

研究者らによると、22年後の33歳時点での最終学歴まで予測できてしまうそうです。

また、この「LLMによる作文分析」、先生の評価、遺伝子情報の三つを組み合わせると、知能テストに匹敵するレベルまで予測精度が向上することも分かりました。
子どもが書く短い文章には、その子の認知能力などに関する膨大な情報が隠されているということが示唆された形です。

これまで生徒の将来を予測する最も確実な方法は先生による評価だと考えられてきましたが、今後は多様なアプローチが手段になるかもしれません。

こうした発見は、プライバシーや公平性に関する新たな議論の必要性も示します。一方で、個別指導の可能性を広げるものでもあります。

### Xのポスト

### 関連記事

[実在する人間1052人の態度と行動をAIでモデル化　インタビューベースのエージェントが人間の回答を85%再現](https://ai-data-base.com/archives/80107)

## 第2位　「大事なことなので2回言いました」のような現象

```
Prompt Repetition Improves Non-Reasoning LLMs
https://doi.org/10.48550/arXiv.2512.14982
Yaniv Leviathan, Matan Kalman, Yossi Matias
（Google Research）
```

Googleの研究者らが短い論文で「LLMに質問するとき、同じ質問を2回続けて入力するだけで、LLMの回答精度が上がる」という発見を報告しています。
あまりに単純すぎて見落とされていた改善策が、主要なモデルすべてで一貫して効果を発揮したという点が、この研究の意外性です。

なぜこれが効くのか。
LLMは文章を前から順番に読んでいくため、たとえば選択肢が先にあって質問が後にあると、選択肢を読んでいる時点ではまだ何を聞かれているのかわからない。
しかし同じ内容を2回繰り返すと、2回目を処理するときには1回目の全体像がすでに頭に入っているので、文脈を完全に把握した状態で答えられる、ということです。
（言われてみれば極めてシンプルな話）

このテクニックの価値は、手間もコストも最低限という点です。
（「ステップバイステップで考えて」といった指示は効果的なこともあるものの、時間もコストもかかる）

なお、最近の高性能な推論モデルは、訓練の過程で自然と「質問内容の一部を繰り返してから答える癖」を身につけているとのことです。
この手法は、そうした有用な動作を人間側が先回りして与えるものとも言えます。

### Xのポスト

### 関連記事

[LLMの推論能力は単純に文脈を繰り返し提示するだけでも大幅に向上　最大で30%改善](https://ai-data-base.com/archives/76967)

## 第1位　100万人の仮想住民で都市を回す

```
CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation
https://doi.org/10.48550/arXiv.2506.21805
Nicolas Bougie, Narimasa Watanabe
（Woven by Toyota）
```

LLMで人間のように行動する仮想的な住民を最大100万人分作り出し、都市全体をシミュレーションするシステムを作ったとのこと。
ウーブン・バイ・トヨタの研究者らによる報告です。

研究者らは、AIに人格や記憶、欲求、長期的な目標を与えることに成功。
お腹が空いたり疲れたりといった基本的な欲求に加えて、社会的なつながりといった高次の欲求も持っています。
And 仮想住民一人一人が過去の経験を覚えていて、それに基づいて行動を決めます。

東京を舞台に設定し1000人分のシミュレーションを行ったところ、平日の通勤ラッシュや週末のレジャー行動、買い物パターンなどが、日本政府の実際の統計データと非常によく一致していたそうです。
渋谷の人混みの分布や、どの店が人気になるかの予測も、現実のデータとかなり近い結果が得られています。

なお、こうしたシミュレーションでは、学習データに含まれる偏見や先入観が反映される可能性があるリスクに気を付けなければいけません。
とはいえ、都市計画や災害対策、商業施設の立地計画など、実際に人を使った実験では難しい様々なシナリオを安全かつ低コストで検証できるツールとして期待されています。

また、仮想住民に”自己実現”など最高位の欲求を実装することも見据えているそうです。

### Xのポスト

### 関連記事

[現実における人間の多様性に対応したLLMペルソナ設計手法の検証](https://ai-data-base.com/archives/88247)

## まとめ

2025年は、AIの性能だけでなく「どう振る舞わせるか」「どこで役立てるか」まで含めて、論点が広がっていった年だったように見えます。

なお、本記事に掲載した以外にも、大変興味深い研究を多く取り上げさせていただいた1年でした。

来年も、最前線の科学技術を一緒に追っていきましょう。
